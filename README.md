# B-Distill

The codes for our paper **Balanced Knowledge Distillation for Large Language Models with Mix-of-Experts**.

## üìñ Overview

This repository contains the official implementation of **B-Distill**, which is a *moe-to-moe* distillation framework for LLMs.

## üèóÔ∏è Framework

![](./images/framework.jpg)

## üìÅ Project Structure

```
B-Distill/
‚îú‚îÄ‚îÄ üìÇ checkpoints/             # Pre-trained model checkpoints
‚îú‚îÄ‚îÄ üìÇ data.zip/                    # Dataset files
‚îÇ   ‚îú‚îÄ‚îÄ alpacmygavel
‚îÇ   ‚îú‚îÄ‚îÄ dolly
‚îÇ   ‚îú‚îÄ‚îÄ GenMedGPT
‚îÇ   ‚îú‚îÄ‚îÄ self-inst
‚îÇ   ‚îú‚îÄ‚îÄ uinst
‚îÇ   ‚îú‚îÄ‚îÄ us_terms
‚îÇ   ‚îî‚îÄ‚îÄ vicuna
‚îú‚îÄ‚îÄ üìÇ images/                  # Pictures in readme
‚îú‚îÄ‚îÄ üìÇ outputs/                 # saved model checkpoints  
‚îú‚îÄ‚îÄ üìÇ peft/                    # lightweight training packages, with the implementation of moe
‚îú‚îÄ‚îÄ üìÇ scripts/                 # training scripts
‚îú‚îÄ‚îÄ üìÇ src/                     # Source code
‚îÇ   ‚îú‚îÄ‚îÄ distillation_trainer.py # trainer for distillation
‚îÇ   ‚îî‚îÄ‚îÄ losses.py               # distillation losses
‚îú‚îÄ‚îÄ build_dataset.py            # dataset scripts
‚îú‚îÄ‚îÄ moe-distill.py              # distill scripts
‚îú‚îÄ‚îÄ eval_rouge_l.py             # evaluate scripts
‚îú‚îÄ‚îÄ requirements.txt            # Dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

## üîß Requirements

### Hardware

- 8 * NVIDIA A100 GPUs (or equivalent)implementation

### Software

- Python 3.12
- PyTorch >= 1.13.1
- CUDA-compatible GPU drivers

## ‚ö° Quick Start

### 1. Installation

Install dependencies:

```bash
pip install -r requirements.txt
```

### 2. Data and Model Preparation

Extract the provided datasets:

```bash
unzip data.zip
```

Download the next models in the **checkpoints** folder:

```
llama-3.1-8b
llama-3.2-1b
qwen3-4b
qwen3-0.6b
```

### 3. Training the teacher model

Train the teacher model on Dolly:

```bash
chmod 777 -R ./
./scripts/fine-tuning-llama3-8b.sh
./scripts/fine-tuning-qwen3-4b.sh
```

### 4. Training the student model (SFT)

Train the student model on Dolly:

```
./scripts/fine-tuning-llama3-1b.sh
./scripts/fine-tuning-qwen0.6b.sh
```

### 5. Distill the student model with B-Distill

Run the next scripts for our B-Distill:

```bash
./scripts/run_expert_distill_llama3_1b.sh
./scripts/run_expert_distill_qwen3.sh
```

### 6. Evaluation

Run the next scripts for evalutaion:

```bash
./scripts/eval_llama3.sh
./scripts/eval_qwen3.sh
```


### 7. Ablation experiments

Remove the **Monte Carlo Expert Exploration**:

```
./scripts/ablation_methoda_llama3.sh
./scripts/ablation_methoda_qwen3.sh
```

Remove the **Entropy-Aware Router Distillation**:

```
./scripts/ablation_methodb_llama3.sh
./scripts/ablation_methodb_qwen3.sh
```

##  üòÄ License

This project is licensed under the MIT License.

## üôè Acknowledgments

We thank the authors of the baseline methods and dataset providers for making their code and data available.
